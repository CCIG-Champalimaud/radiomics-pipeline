{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of ISUC scores using radiomic features from three different sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tqdm import tqdm,trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diagnostics' 'exponential' 'gradient' 'lbp-2D' 'log-sigma-1-mm-3D'\n",
      " 'logarithm' 'original' 'square' 'squareroot' 'wavelet-HH' 'wavelet-HL'\n",
      " 'wavelet-LH' 'wavelet-LL']\n"
     ]
    }
   ],
   "source": [
    "diagnost_keys = [\n",
    "    'diagnostics_Versions_PyRadiomics', 'diagnostics_Versions_Numpy', \n",
    "    'diagnostics_Versions_SimpleITK', 'diagnostics_Versions_PyWavelet', \n",
    "    'diagnostics_Versions_Python', 'diagnostics_Image-original_Hash',\n",
    "    'diagnostics_Image-original_Dimensionality',  \n",
    "    'diagnostics_Mask-original_Hash', 'diagnostics_Mask-original_VoxelNum',\n",
    "    'diagnostics_Mask-original_VolumeNum']\n",
    "\n",
    "metadata_keys = [\n",
    "    'path', 'lesion_id', 'sequence_id', 'class', \n",
    "    'config_file', 'registration', 'mask_resampling']\n",
    "\n",
    "with open(\"radiomic_features/features_no_reg.csv\", 'r') as o:\n",
    "    header = next(o).strip().split(',')\n",
    "    output_dict = {\n",
    "        k:[] for k in header \n",
    "        if (k not in metadata_keys) and (k not in diagnost_keys)}\n",
    "    diagnost_dict = {k:[] for k in diagnost_keys}\n",
    "    metadata_dict = {k:[] for k in metadata_keys}\n",
    "    for line in o:\n",
    "        line = line.strip().split(',')\n",
    "        for i in range(len(line)):\n",
    "            k = header[i]\n",
    "            if k in output_dict:\n",
    "                output_dict[k].append(float(line[i]))\n",
    "            elif k in diagnost_keys:\n",
    "                diagnost_dict[k].append(line[i])\n",
    "            elif k in metadata_dict:\n",
    "                metadata_dict[k].append(line[i])\n",
    "\n",
    "metadata_dict[\"study_id\"] = [\n",
    "    '_'.join(x.split(os.sep)[-1].split('_')[:-1]) for x in metadata_dict['path']]\n",
    "\n",
    "print(np.unique([x.split('_')[0] for x in output_dict.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_data = {}\n",
    "for i in range(len(metadata_dict[\"study_id\"])):\n",
    "    study_id = metadata_dict[\"study_id\"][i]\n",
    "    lesion_id = int(float(metadata_dict[\"lesion_id\"][i]))\n",
    "    sequence_id = int(float(metadata_dict[\"sequence_id\"][i]))\n",
    "    cl = int(float(metadata_dict[\"class\"][i]))\n",
    "    data_row = []\n",
    "    for k in output_dict:\n",
    "        data_row.append(output_dict[k][i])\n",
    "    data_row = np.array(data_row)\n",
    "    if study_id not in hierarchical_data:\n",
    "        hierarchical_data[study_id] = {}\n",
    "    if lesion_id not in hierarchical_data[study_id]:\n",
    "        hierarchical_data[study_id][lesion_id] = {}\n",
    "    hierarchical_data[study_id][lesion_id][sequence_id] = {\n",
    "        \"data\":data_row,\"class\":cl}\n",
    "\n",
    "sequence_ids = [0,1,2]\n",
    "data_dict = {k:[] for k in sequence_ids}\n",
    "data_dict[\"class\"] = []\n",
    "for study_id in hierarchical_data:\n",
    "    for lesion_id in hierarchical_data[study_id]:\n",
    "        cur_s_ids = hierarchical_data[study_id][lesion_id].keys()\n",
    "        # check that all sequences are available\n",
    "        set_diff = set.difference(set(cur_s_ids),set(sequence_ids))\n",
    "        if len(set_diff) == 0:\n",
    "            for s in sequence_ids:\n",
    "                v = hierarchical_data[study_id][lesion_id][s][\"data\"]\n",
    "                cl = hierarchical_data[study_id][lesion_id][s][\"class\"]\n",
    "                data_dict[s].append(v)\n",
    "            data_dict[\"class\"].append(cl)\n",
    "for k in data_dict:\n",
    "    data_dict[k] = np.array(data_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up different steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection + standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveCorrelatedVariables:\n",
    "    def __init__(self,threshold=0.8):\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        self.corr_coef = np.abs(np.corrcoef(X.T))\n",
    "        self.var_to_remove = []\n",
    "        for i in range(self.corr_coef.shape[0]):\n",
    "            if i not in self.var_to_remove:\n",
    "                high_coef = np.where(\n",
    "                    self.corr_coef[i,:]>self.threshold)[0]\n",
    "                high_coef = [x for x in high_coef if x != i]\n",
    "                if len(high_coef) > 0:\n",
    "                    # highest mean correlation\n",
    "                    highest_mc = np.mean(self.corr_coef[i])\n",
    "                    rm = []\n",
    "                    for j in high_coef:\n",
    "                        mc = np.mean(self.corr_coef[i])\n",
    "                        if mc > highest_mc:\n",
    "                            rm.append(j)\n",
    "                    if len(rm) < len(high_coef):\n",
    "                        self.var_to_remove.append(i)\n",
    "                    else:\n",
    "                        self.var_to_remove.extend(rm)\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        return X[:,self.var_to_remove]\n",
    "    \n",
    "    def fit_transform(self,X,y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,GridSearchCV\n",
    "from sklearn.feature_selection import SelectFpr\n",
    "from sklearn.utils import compute_sample_weight,compute_class_weight\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV,SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier,BernoulliRBM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import VarianceThreshold,SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "def create_preprocessing_pipeline(C):\n",
    "    return Pipeline([\n",
    "        ('remove_nzv', VarianceThreshold()),\n",
    "        ('remove_correlated_variables', RemoveCorrelatedVariables(0.95)),\n",
    "        ('standardization',StandardScaler()),\n",
    "        ('select_from_model', SelectFromModel(\n",
    "            LinearSVC(\n",
    "                penalty=\"l1\",max_iter=50000,C=C,dual=False,\n",
    "                class_weight=\"balanced\",tol=1e-3)))\n",
    "    ])\n",
    "\n",
    "def binarize(x):\n",
    "    return np.where(x <= 1,0,1)\n",
    "\n",
    "model_dict_mc = {}\n",
    "model_dict_binary = {}\n",
    "\n",
    "skf = StratifiedKFold(5,random_state=42,shuffle=True)\n",
    "cl = np.array(data_dict[\"class\"]) - 2\n",
    "\n",
    "model_id = \"extra_trees\"\n",
    "model_params_dict = {\n",
    "    \"extra_trees\":{\n",
    "        \"model\":ExtraTreeClassifier,\n",
    "        \"params\":{\"class_weight\":\"balanced\"},\n",
    "        \"cv_params\":{'splitter':('random','best'),\n",
    "                     'min_samples_split':(2,4)} \n",
    "    },\n",
    "    \"elastic\":{\n",
    "        \"model\":SGDClassifier,\n",
    "        \"params\":{\"penalty\":\"elasticnet\",\n",
    "                  \"n_jobs\":1,\n",
    "                  \"class_weight\":\"balanced\",\n",
    "                  \"loss\":\"modified_huber\"},\n",
    "        \"cv_params\":{\"alpha\":[0.001,0.01,0.1,0.5]}\n",
    "    },\n",
    "    \"l2\":{\n",
    "        \"model\":LogisticRegressionCV,\n",
    "        \"params\":{\"penalty\":\"l2\",\n",
    "                  \"max_iter\":1000,\n",
    "                  \"class_weight\":\"balanced\",\n",
    "                  \"n_jobs\":1},\n",
    "        \"cv_params\":{\"Cs\":[10,20,30]}\n",
    "    },\n",
    "    \"mlp\":{\n",
    "        \"model\":MLPClassifier,\n",
    "        \"params\":{\"max_iter\":1000},\n",
    "        \"cv_params\":{\"alpha\":[0.01,0.1,0.5,1],\n",
    "                     \"hidden_layer_sizes\":[(100,),(100,100),(100,100,100),(100,100,100,100)]}\n",
    "    },\n",
    "    \"knn\":{\n",
    "        \"model\":KNeighborsClassifier,\n",
    "        \"params\":{},\n",
    "        \"cv_params\":{\"n_neighbors\":[5,7,10],\n",
    "                     \"weights\":[\"uniform\",\"distance\"]}\n",
    "    },\n",
    "    \"rf\":{\n",
    "        \"model\":RandomForestClassifier,\n",
    "        \"params\":{\"oob_score\":False,\"class_weight\":\"balanced\"},\n",
    "        \"cv_params\":{\"n_estimators\":[25,50]}\n",
    "    },\n",
    "    \"grad_boost\":{\n",
    "        \"model\":GradientBoostingClassifier,\n",
    "        \"params\":{\"subsample\":0.5},\n",
    "        \"cv_params\":{\"n_estimators\":[25,50]}\n",
    "    },\n",
    "    \"xgb\":{\n",
    "        \"model\":xgb.XGBClassifier,\n",
    "        \"params\":{\"tree_method\":\"gpu_hist\"},\n",
    "        \"cv_params\":{\"n_estimators\":[25,50,100]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb T2W fold = 0\n",
      "xgb T2W fold = 1\n",
      "xgb T2W fold = 2\n",
      "xgb T2W fold = 3\n",
      "xgb T2W fold = 4\n",
      "xgb ADC fold = 0\n",
      "xgb ADC fold = 1\n",
      "xgb ADC fold = 2\n",
      "xgb ADC fold = 3\n",
      "xgb ADC fold = 4\n",
      "xgb HBV fold = 0\n",
      "xgb HBV fold = 1\n",
      "xgb HBV fold = 2\n",
      "xgb HBV fold = 3\n",
      "xgb HBV fold = 4\n",
      "xgb T2W:ADC fold = 0\n",
      "xgb T2W:ADC fold = 1\n",
      "xgb T2W:ADC fold = 2\n",
      "xgb T2W:ADC fold = 3\n",
      "xgb T2W:ADC fold = 4\n",
      "xgb T2W:HBV fold = 0\n",
      "xgb T2W:HBV fold = 1\n",
      "xgb T2W:HBV fold = 2\n",
      "xgb T2W:HBV fold = 3\n",
      "xgb T2W:HBV fold = 4\n",
      "xgb ADC:HBV fold = 0\n",
      "xgb ADC:HBV fold = 1\n",
      "xgb ADC:HBV fold = 2\n",
      "xgb ADC:HBV fold = 3\n",
      "xgb ADC:HBV fold = 4\n",
      "xgb T2W:ADC:HBV fold = 0\n",
      "xgb T2W:ADC:HBV fold = 1\n"
     ]
    }
   ],
   "source": [
    "models_to_train = [\n",
    "    \"xgb\",\n",
    "    \"grad_boost\",\n",
    "    \"rf\",\n",
    "    \"knn\",\n",
    "    \"mlp\",\n",
    "    \"extra_trees\",\n",
    "    \"elastic\",\n",
    "    \"l2\",\n",
    "]\n",
    "\n",
    "C_preproc = 0.5\n",
    "remove_outliers = False\n",
    "\n",
    "for model_id in models_to_train:\n",
    "    model_params = model_params_dict[model_id]\n",
    "    cv_params = model_params[\"cv_params\"]\n",
    "    classifier_fn = model_params[\"model\"]\n",
    "    classifier_params = model_params[\"params\"]\n",
    "\n",
    "    model_dict_mc[model_id] = {}\n",
    "    model_dict_binary[model_id] = {}\n",
    "\n",
    "    for comb in [[0],[1],[2],[0,1],[0,2],[1,2],[0,1,2]]:\n",
    "        data = np.concatenate(\n",
    "            [data_dict[i] for i in comb],axis=1)\n",
    "\n",
    "        cr_key = ':'.join([[\"T2W\",\"ADC\",\"HBV\"][i] for i in comb])\n",
    "        model_dict_mc[model_id][cr_key] = []\n",
    "        model_dict_binary[model_id][cr_key] = []\n",
    "        splits = skf.split([i for i in range(len(cl))],cl)\n",
    "        for i,(train_split,val_split) in enumerate(splits):\n",
    "            print(model_id,cr_key,\"fold =\",i)\n",
    "            preproc_pipeline = create_preprocessing_pipeline(C_preproc)\n",
    "            # data\n",
    "            training_set_original = data[train_split,:]\n",
    "            val_set_original = data[val_split,:]\n",
    "\n",
    "            # class\n",
    "            training_y = cl[train_split]\n",
    "            val_y = cl[val_split]\n",
    "            training_y_binary = binarize(training_y)\n",
    "            val_y_binary = binarize(val_y)\n",
    "\n",
    "            preproc_pipeline.fit(training_set_original,training_y)\n",
    "            training_set = preproc_pipeline.transform(training_set_original)\n",
    "            val_set = preproc_pipeline.transform(val_set_original)\n",
    "            \n",
    "            if remove_outliers == True:\n",
    "                rm_outliers = LocalOutlierFactor(novelty=True)\n",
    "                rm_outliers.fit(training_set)\n",
    "                inliers = rm_outliers.predict(training_set)\n",
    "                inliers = np.where(inliers < 0,False,True)\n",
    "                training_set = training_set[inliers,:]\n",
    "                training_y = training_y[inliers]\n",
    "            else:\n",
    "                inliers = []\n",
    "\n",
    "            classifier = GridSearchCV(\n",
    "                classifier_fn(**classifier_params),cv_params,n_jobs=8,\n",
    "                scoring=\"f1_macro\")\n",
    "\n",
    "            classifier.fit(training_set,training_y)\n",
    "\n",
    "            training_pred = classifier.predict(training_set)\n",
    "            training_prob = classifier.predict_proba(training_set)\n",
    "            \n",
    "            val_pred = classifier.predict(val_set)\n",
    "            val_prob = classifier.predict_proba(val_set)\n",
    "            \n",
    "            model_dict_mc[model_id][cr_key].append(\n",
    "                {\"classification_report_train\":classification_report(\n",
    "                    training_y,training_pred,output_dict=True,zero_division=0),\n",
    "                 \"classification_report_val\":classification_report(\n",
    "                     val_y,val_pred,output_dict=True,zero_division=0),\n",
    "                 \"auc_train\":roc_auc_score(training_y,training_prob,\n",
    "                                           multi_class='ovo'),\n",
    "                 \"auc_val\":roc_auc_score(val_y,val_prob,\n",
    "                                         multi_class='ovo'),\n",
    "                 \"model\":classifier,\n",
    "                 \"preproc_pipeline\":preproc_pipeline,\n",
    "                 \"train_split\":train_split,\n",
    "                 \"val_split\":val_split,\n",
    "                 \"inliers\":inliers})\n",
    "            \n",
    "            preproc_pipeline = create_preprocessing_pipeline(C_preproc)\n",
    "            preproc_pipeline.fit(training_set_original,training_y_binary)\n",
    "            training_set = preproc_pipeline.transform(training_set_original)\n",
    "            val_set = preproc_pipeline.transform(val_set_original)\n",
    "            \n",
    "            if remove_outliers == True:\n",
    "                rm_outliers.fit(training_set)\n",
    "                inliers = rm_outliers.predict(training_set)\n",
    "                inliers = np.where(inliers < 0,False,True)\n",
    "                training_set = training_set[inliers,:]\n",
    "                training_y_binary = training_y_binary[inliers]\n",
    "            else:\n",
    "                inliers = []\n",
    "\n",
    "            classifier = GridSearchCV(\n",
    "                classifier_fn(**classifier_params),cv_params,n_jobs=8,\n",
    "                scoring=\"f1_macro\")\n",
    "\n",
    "            classifier.fit(training_set,training_y_binary)\n",
    "\n",
    "            training_pred = classifier.predict(training_set)\n",
    "            training_prob = classifier.predict_proba(training_set)\n",
    "            try: training_prob = training_prob[:,1]\n",
    "            except: pass\n",
    "\n",
    "            val_pred = classifier.predict(val_set)\n",
    "            val_prob = classifier.predict_proba(val_set)\n",
    "            try: val_prob = val_prob[:,1]\n",
    "            except: pass\n",
    "\n",
    "            model_dict_binary[model_id][cr_key].append(\n",
    "                {\"classification_report_train\":classification_report(\n",
    "                    training_y_binary,training_pred,output_dict=True,zero_division=0),\n",
    "                 \"classification_report_val\":classification_report(\n",
    "                     val_y_binary,val_pred,output_dict=True,zero_division=0),\n",
    "                 \"auc_train\":roc_auc_score(training_y_binary,training_prob),\n",
    "                 \"auc_val\":roc_auc_score(val_y_binary,val_prob),\n",
    "                 \"model\":classifier,\n",
    "                 \"preproc_pipeline\":preproc_pipeline,\n",
    "                 \"train_split\":train_split,\n",
    "                 \"val_split\":val_split,\n",
    "                 \"inliers\":inliers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "ks = [\"f1-score\",\"recall\",\"precision\",\"auc\"]\n",
    "metric_dict = {\"model\":[],\"mods\":[],\"fold\":[]}\n",
    "for k in ks:\n",
    "    metric_dict[k+\"_val\"] = []\n",
    "    metric_dict[k+\"_train\"] = []\n",
    "    \n",
    "for model_id in model_dict_binary:\n",
    "    for k in model_dict_binary[model_id]:\n",
    "        cr = model_dict_binary[model_id][k]\n",
    "        for i,c in enumerate(cr):\n",
    "            cr_train = c[\"classification_report_train\"][\"1\"]\n",
    "            cr_val = c[\"classification_report_val\"][\"1\"]\n",
    "            metric_dict[\"auc_train\"].append(c[\"auc_train\"])\n",
    "            metric_dict[\"auc_val\"].append(c[\"auc_val\"])\n",
    "            metric_dict[\"model\"].append(model_id)\n",
    "            metric_dict[\"mods\"].append(k)\n",
    "            metric_dict[\"fold\"].append(i)\n",
    "            for metric_k in cr_train:\n",
    "                if metric_k in ks:\n",
    "                    metric_dict[metric_k+\"_train\"].append(cr_train[metric_k])\n",
    "            for metric_k in cr_val:\n",
    "                if metric_k in ks:\n",
    "                    metric_dict[metric_k+\"_val\"].append(cr_val[metric_k])\n",
    "\n",
    "binary_metrics_df = pd.DataFrame(metric_dict)\n",
    "\n",
    "sb.set(style=\"darkgrid\")\n",
    "\n",
    "sb.pointplot(\n",
    "    data=binary_metrics_df,\n",
    "    x=\"auc_val\",y=\"model\",hue=\"mods\",join=False,dodge=0.7,\n",
    "    size=0.05,ci=100)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "57907a0bf32e194a840802e99a9c21023814e75299a7a09c1397f9b831ab2e38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
